"""
ç›®å‰å°±åˆ°æ­¤ä¸ºæ­¢äº†å§ï¼Œæ—¶é—´ä¸å¤šäº†ï¼ˆ8æœˆ6å·æ™šï¼‰ï¼Œè¿˜æœ‰å…¶ä»–äº‹è¦å¿™
ä½¿ç”¨äº†requestså’Œetreeï¼Œurllibå’ŒBeautifulSoupè¿˜æ²¡ç”¨
ä¸ä¼šä½¿ç”¨**æ­£åˆ™è¡¨è¾¾å¼**å°†çˆ¬å–çš„å†…å®¹ç­›é€‰ï¼Œå­¦ä¼šåå¯ä»¥å°è¯•æå–ç”µå½±ç§ç±»ã€ä¸Šæ˜ æ—¶é—´
è¿˜æœ‰å¯ä»¥å°è¯•é€šè¿‡æ‰“å¼€ç”µå½±çš„é“¾æ¥è·å–æ›´å®Œæ•´çš„ä¿¡æ¯ï¼Œå¦‚æ¼”å‘˜åˆ—è¡¨
"""

import requests
# from bs4 import BeautifulSoup   # æ­¤æ–‡ä»¶å¼ƒç”¨BeautifulSoup
from lxml.html import etree  # æ–°ç‰ˆlxmlä¸­å¦‚æœè¦ä½¿ç”¨etreeï¼Œéœ€è¦ä½¿ç”¨lxml.html
import pandas as pd  # csvæ–‡ä»¶ç”¨

number_lists = []
title_lists = []
title_origin_lists = []
link_lists = []
score_lists = []
staff_lists = []
other_lists = []
quote_lists = []


def getpage(page):
    url = 'https://movie.douban.com/top250?start={}'.format((page - 1) * 25)  # formatå‡½æ•°å®ç°ç¿»é¡µ
    # headeråœ¨F12ä¸­çš„Networkä¸­ï¼Œå†åˆ·æ–°ä¸€ä¸‹å¯ä»¥æ‰¾åˆ°
    header = {
        'User-Agent': "Mozilla/5.0(Windows NT 10.0;Win64;x64) AppleWebKit/537.36(KHTML, likeGecko) Chrome/75.0.3770.142Safari/537.36"}
    req = requests.get(url, headers=header)  # è·å–URL
    # print(req.text)  # çœ‹ç½‘é¡µæºç 
    return req.text
    # bs = BeautifulSoup(html, features="lxml")
    # texts = bs.find_all('ol', class_="grid_view")
    # print(texts)  # BeautifulSoupæå–ï¼Œå†…å®¹ç±»ä¼¼äºF12ä¸­çš„å†…å®¹


def parse(text):  # è§£ææ•°æ®
    contents_staff = []
    contents_other = []
    html = etree.HTML(text)  # åˆå§‹åŒ–&æ ‡å‡†åŒ–
    # xpath()è¿”å›**åˆ—è¡¨**
    # ä¸€å¼€å§‹ä¸çŸ¥é“æ˜¯åˆ—è¡¨ï¼Œåˆappendåˆ°å¦ä¸€ä¸ªåˆ—è¡¨é‡Œï¼Œå¯¼è‡´æœ‰ä¸¤å±‚ä¸­æ‹¬å·ï¼Œforå¾ªç¯åªèƒ½ç®—ä¸€ä¸ªï¼Œä½¿ç”¨replaceä¼šå¯¼è‡´æ— æ³•è¯†åˆ«åˆ—è¡¨ï¼Œåªèƒ½ç®—str
    # å› ä¸ºæºç å«æœ‰å¤šä¸ªdivå’Œspanï¼Œéœ€è¦æ ‡æ³¨åºå·ï¼›éƒ¨åˆ†éœ€è¦text()ï¼Œå¦åˆ™ä¼šæŠ¥é”™
    contents_title = html.xpath('//div[2]/div[@class="hd"]/a/span[1]/text()')
    contents_title_origin = html.xpath('//div[2]/div[@class="hd"]/a/span[2]/text()')
    contents_url = html.xpath('//div[2]/div[@class="hd"]/a/@href')
    contents_score = html.xpath('//div[2]/div[@class="bd"]/div/span[@class="rating_num"]/text()')
    # æ­¤å¤„contents_score = html.xpath('//div[2]/div[@class="bd"]/div[1]/span[2]/text()')ä¹Ÿå¯ä»¥ï¼Œç”¨åºå·ä»£æ›¿ï¼Œæ³¨æ„ä»1å¼€å§‹
    contents_quote = html.xpath('//div[2]/div[@class="bd"]/p[2]/span[@class="inq"]/text()')
    contents_info = html.xpath('//div[2]/div[@class="bd"]/p[1]/text()')  # æ—¥æœŸä¹Ÿæ˜¯æœ‰\xa0/\xa0å‡ºç°ï¼Œç›®å‰æ— æ³•è§£å†³
    # print(contents_info)
    n = 0
    for data in range(len(contents_info)):
        if n % 2 == 0:
            contents_staff.extend(contents_info[n])
        else:
            contents_other.extend(contents_info[n])
        n += 1
    # print(contents_staff)
    # print(contents_other)
    # è¿™é‡Œçš„forä¼šæŠŠæ¯ä¸€ä¸ªå…ƒç´ éƒ½éå†ï¼Œä¸€æ®µè¯çš„æ¯ä¸€ä¸ªå­—éƒ½ä¼šè¢«åˆ†å¼€ï¼Œå°±å¾ˆç¦»è°±
    contents_txt = zip(contents_title, contents_title_origin, contents_url, contents_score, contents_staff,
                       contents_other, contents_quote)
    # zip()å‡½æ•°ï¼Œcontents=[(contents_title,contents_urlï¼Œ contents_score),()...]ï¼Œæ˜¯å…ƒç»„
    return contents_txt, contents_title, contents_title_origin, contents_url, contents_score, \
           contents_staff, contents_other, contents_quote


def write(data):
    raw_contents = getpage(data)  # æ­¤å¤„æˆ‘é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼šåºå·ç‰¹åˆ«å¤šï¼Œç»“æœå‘ç°å†…å®¹ä½¿ç”¨çš„æ˜¯æ²¡æœ‰parseè¿‡çš„æ•°æ®ğŸ˜“
    contents, titles, titles_origin, urls, scores, staff, other, quote = parse(raw_contents)
    title_lists.extend(titles)  # å°†å¤šä¸ªç½‘é¡µçš„å†…å®¹æ‹¼æ¥åˆ°ä¸€ä¸ªåˆ—è¡¨
    title_origin_lists.extend(titles_origin)
    link_lists.extend(urls)
    score_lists.extend(scores)
    staff_lists.extend(staff)
    other_lists.extend(other)
    quote_lists.extend(quote)
    print(len(title_lists))
    print(len(title_origin_lists))
    print(len(link_lists))
    print(len(score_lists))
    print(len(staff_lists))
    print(len(other_lists))
    print(len(quote_lists))
    a = 0
    # æ–¹æ³•ä¸€(txt)ï¼š
    with open('douban_movie_top250(txt).txt', 'a', encoding='utf-8') as f:
        for content_tuple in contents:
            f.write("No " + str((i - 1) * 25 + a + 1) + ":\t")  # åºå·
            a += 1
            # ä¸ä¼šæ­£åˆ™è¡¨è¾¾å¼ï¼Œä¸ä¼šåˆ†å‰²ï¼Œå¯¹\xa0/\xa0å‡ºç°ç­‰é—®é¢˜ä½¿ç”¨æ¯”è¾ƒåŸå§‹çš„replaceå‡½æ•°
            # # æˆ‘ä¸æ˜¯è¯ç¥ç«Ÿç„¶æ²¡æœ‰quoteï¼Œ ä¹Ÿä¸çŸ¥é“å…ƒç»„æ€ä¹ˆåŠ ï¼Œéƒ½å¤±è´¥äº†ï¼Œæ‡’å¾—ç®¡äº†
            content1 = str(content_tuple).replace('(', '')
            content2 = content1.replace(')', '')
            content3 = content2.replace('\'', '')
            content4 = content3.replace(',', ' ')
            content5 = content4.replace('\\xa0', '')
            content6 = content5.replace('/', '', 1)
            content7 = content6.replace('\\n                            ', '')
            content8 = content7.replace("\\n", '')
            f.write(content8.strip() + '\n')


for i in range(1, 10):
    write(i)
    print(f"æ­£åœ¨çˆ¬å–ç¬¬{i}é¡µ")  # ç®€åŒ–formatå‡½æ•°

# æ–¹æ³•äºŒ(csv)ï¼š
b = 0
quote_lists.insert(134, ' ')  # æˆ‘ä¸æ˜¯è¯ç¥ç«Ÿç„¶æ²¡æœ‰quoteï¼Œå®³æˆ‘æäº†åŠå¤©ï¼›txtçƒ‚æ³¥æ‰¶ä¸ä¸Šå¢™ï¼Œæ‡’å¾—ç®¡äº†ï¼Œè€Œä¸”zipå‡½æ•°ç”¨çš„æ˜¯å…ƒç»„ï¼Œéš¾é¡¶
for aa in range(len(title_origin_lists)):
    title_origin_lists[aa].replace('?', '')
    title_origin_lists[aa].replace('/', '',1)
for x in range(len(title_lists)):
    number_lists.append(b + 1)
    b += 1
# contents_info = {'number': number_lists, 'name': title_lists, 'name_origin': title_origin_lists, 'link': link_lists,
#                  'score': score_lists, 'staff': staff_lists, 'other': other_lists, 'quote': quote_lists}
contents_info = {'number': number_lists, 'name': title_lists, 'name_origin': title_origin_lists, 'link': link_lists,
                 'score': score_lists, 'quote': quote_lists}
print(contents_info)
frame = pd.DataFrame(contents_info)  # csvæ–‡ä»¶
frame.to_csv(r'D:\programme_study\Python_work\crawler\douban_movie_top250(csv).csv')  # ä¹±ç çš„è¯ï¼Œè®°äº‹æœ¬æ‰“å¼€ï¼Œè½¬ç ANSI
print(frame)
